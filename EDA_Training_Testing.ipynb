{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0t6WTKyNxyivJ1KQcbtUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladimirVladetic/OSiRV-RoadSignDetection/blob/main/EDA_Training_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! pip install scikit-plot"
      ],
      "metadata": {
        "id": "vjDmNcTO6U8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, Input\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import zipfile\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img\n",
        "import shutil\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from skimage import feature\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from skimage.morphology import erosion, dilation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50, InceptionV3\n",
        "from google.colab import files\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import scikitplot as skplt"
      ],
      "metadata": {
        "id": "CU3F_zwFhm6i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_images_with_zero_dimensions(images_array, class_label_array, bbox_array):\n",
        "    indices_to_remove = []\n",
        "    for i, image in enumerate(images_array):\n",
        "        height, width, channels = image.shape\n",
        "        if height == 32 or width == 32:\n",
        "            indices_to_remove.append(i)\n",
        "    images_array = np.delete(images_array, indices_to_remove, axis=0)\n",
        "    class_label_array = np.delete(class_label_array, indices_to_remove, axis=0)\n",
        "    bbox_array = np.delete(bbox_array, indices_to_remove, axis=0)\n",
        "\n",
        "    return images_array, class_label_array, bbox_array, indices_to_remove"
      ],
      "metadata": {
        "id": "InaWvpg0EpF1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_images(images, bboxes):\n",
        "    cropped_images = []\n",
        "    for image, bbox in zip(images, bboxes):\n",
        "\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        xmin = max(0, int(xmin))\n",
        "        ymin = max(0, int(ymin))\n",
        "        xmax = min(image.shape[1], int(xmax))\n",
        "        ymax = min(image.shape[0], int(ymax))\n",
        "        cropped_image = image[ymin:ymax, xmin:xmax, :]\n",
        "        cropped_images.append(cropped_image)\n",
        "\n",
        "    return cropped_images"
      ],
      "metadata": {
        "id": "1k7Ql2qnd63E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_edge(images, height, width):\n",
        "    features = []\n",
        "    for var_img  in tqdm(images):\n",
        "\n",
        "        edge_channels = [feature.canny(var_img[:, :, i]) for i in range(3)]\n",
        "        edges = np.stack(edge_channels, axis=-1)\n",
        "        edges_image = Image.fromarray(edges.astype(np.uint8) * 255)\n",
        "        edges_image = edges_image.resize((height, width), Image.ANTIALIAS)\n",
        "        edges_image = np.array(edges_image)\n",
        "        edges_image = edges_image / 255.0\n",
        "        features.append(edges_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "H7DoHa8WEJmq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_edge_with_padding(images, height, width):\n",
        "    features = []\n",
        "    for var_img  in tqdm(images):\n",
        "\n",
        "        padding_needed = False\n",
        "        if var_img.shape[0] < height or var_img.shape[1] < width:\n",
        "            padding_needed = True\n",
        "        if padding_needed:\n",
        "            pad_height = max(0, height - var_img.shape[0])\n",
        "            pad_width = max(0, width - var_img.shape[1])\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "            var_img = np.pad(var_img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
        "\n",
        "        edge_channels = [feature.canny(var_img[:, :, i]) for i in range(3)]\n",
        "        edges = np.stack(edge_channels, axis=-1)\n",
        "        edges_image = Image.fromarray(edges.astype(np.uint8) * 255)\n",
        "        edges_image = edges_image.resize((height, width), Image.ANTIALIAS)\n",
        "        edges_image = np.array(edges_image)\n",
        "        edges_image = edges_image / 255.0\n",
        "        features.append(edges_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "LGHHFoPTuTAu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_dilation(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "        r, g, b = var_img[:, :, 0], var_img[:, :, 1], var_img[:, :, 2]\n",
        "        dilated_r = dilation(r)\n",
        "        dilated_g = dilation(g)\n",
        "        dilated_b = dilation(b)\n",
        "        dilated_image = np.stack((dilated_r, dilated_g, dilated_b), axis=-1)\n",
        "        dilated_image = (dilated_image * 255).astype(np.uint8)\n",
        "        var_image = Image.fromarray(dilated_image)\n",
        "        var_image = var_image.resize((height, width), Image.ANTIALIAS)\n",
        "        var_image = np.array(var_image) / 255.0\n",
        "        features.append(var_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "j8ABqrGuxT27"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_dilation_with_padding(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "\n",
        "        padding_needed = False\n",
        "        if var_img.shape[0] < height or var_img.shape[1] < width:\n",
        "            padding_needed = True\n",
        "        if padding_needed:\n",
        "            pad_height = max(0, height - var_img.shape[0])\n",
        "            pad_width = max(0, width - var_img.shape[1])\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "            var_img = np.pad(var_img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
        "\n",
        "        r, g, b = var_img[:, :, 0], var_img[:, :, 1], var_img[:, :, 2]\n",
        "        dilated_r = dilation(r)\n",
        "        dilated_g = dilation(g)\n",
        "        dilated_b = dilation(b)\n",
        "        dilated_image = np.stack((dilated_r, dilated_g, dilated_b), axis=-1)\n",
        "        dilated_image = (dilated_image * 255).astype(np.uint8)\n",
        "        var_image = Image.fromarray(dilated_image)\n",
        "        var_image = var_image.resize((height, width), Image.ANTIALIAS)\n",
        "        var_image = np.array(var_image) / 255.0\n",
        "        features.append(var_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "9H2B7OtdxgF9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_erosion(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "        r, g, b = var_img[:, :, 0], var_img[:, :, 1], var_img[:, :, 2]\n",
        "        eroded_r = erosion(r)\n",
        "        eroded_g = erosion(g)\n",
        "        eroded_b = erosion(b)\n",
        "        eroded_image = np.stack((eroded_r, eroded_g, eroded_b), axis=-1)\n",
        "        eroded_image = (eroded_image * 255).astype(np.uint8)\n",
        "        var_image = Image.fromarray(eroded_image)\n",
        "        var_image = var_image.resize((height, width), Image.ANTIALIAS)\n",
        "        var_image = np.array(var_image) / 255.0\n",
        "        features.append(var_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "KTvQoQ6mEN4X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_erosion_with_padding(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "\n",
        "        padding_needed = False\n",
        "        if var_img.shape[0] < height or var_img.shape[1] < width:\n",
        "            padding_needed = True\n",
        "        if padding_needed:\n",
        "            pad_height = max(0, height - var_img.shape[0])\n",
        "            pad_width = max(0, width - var_img.shape[1])\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "            var_img = np.pad(var_img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
        "\n",
        "        r, g, b = var_img[:, :, 0], var_img[:, :, 1], var_img[:, :, 2]\n",
        "        eroded_r = erosion(r)\n",
        "        eroded_g = erosion(g)\n",
        "        eroded_b = erosion(b)\n",
        "        eroded_image = np.stack((eroded_r, eroded_g, eroded_b), axis=-1)\n",
        "        eroded_image = (eroded_image * 255).astype(np.uint8)\n",
        "        var_image = Image.fromarray(eroded_image)\n",
        "        var_image = var_image.resize((height, width), Image.ANTIALIAS)\n",
        "        var_image = np.array(var_image) / 255.0\n",
        "        features.append(var_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "g1TdfYMc7zGr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "        r, g, b = var_img[:, :, 0], var_img[:, :, 1], var_img[:, :, 2]\n",
        "        r_normalized = r / 255.0\n",
        "        g_normalized = g / 255.0\n",
        "        b_normalized = b / 255.0\n",
        "        normalized_image = np.stack((r_normalized, g_normalized, b_normalized), axis=-1)\n",
        "        features.append(normalized_image)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "A-anCTf51oh6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_with_padding(images, height, width):\n",
        "    features = []\n",
        "    for var_img in tqdm(images):\n",
        "        if var_img.shape[0] < height or var_img.shape[1] < width:\n",
        "            pad_height = max(0, height - var_img.shape[0])\n",
        "            pad_width = max(0, width - var_img.shape[1])\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "\n",
        "            var_img = np.pad(var_img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
        "\n",
        "        var_image = Image.fromarray(var_img)\n",
        "        var_image = var_image.resize((width, height), Image.ANTIALIAS)\n",
        "        var_img_resized = np.array(var_image)\n",
        "\n",
        "        normalized_image = var_img_resized / 255.0\n",
        "        features.append(normalized_image)\n",
        "\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "UCbggTwn1r8U"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_contrast(images, height, width):\n",
        "    features = []\n",
        "    for image in tqdm(images):\n",
        "        var_img = Image.fromarray(image, mode='RGB')\n",
        "        r, g, b = var_img.split()\n",
        "        r = ImageOps.equalize(r)\n",
        "        g = ImageOps.equalize(g)\n",
        "        b = ImageOps.equalize(b)\n",
        "        var_img = Image.merge(\"RGB\", (r, g, b))\n",
        "        var_img = var_img.resize((height,width), Image.ANTIALIAS)\n",
        "        var_img = np.array(var_img)\n",
        "        var_img = var_img / 255.0\n",
        "        features.append(var_img)\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "SDr6OwjjERyN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_contrast_with_padding(images, height, width):\n",
        "    features = []\n",
        "    for image in tqdm(images):\n",
        "\n",
        "        padding_needed = False\n",
        "        if image.shape[0] < height or image.shape[1] < width:\n",
        "            padding_needed = True\n",
        "        if padding_needed:\n",
        "            pad_height = max(0, height - image.shape[0])\n",
        "            pad_width = max(0, width - image.shape[1])\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "            image = np.pad(image, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
        "\n",
        "        var_img = Image.fromarray(image, mode='RGB')\n",
        "        r, g, b = var_img.split()\n",
        "        r = ImageOps.equalize(r)\n",
        "        g = ImageOps.equalize(g)\n",
        "        b = ImageOps.equalize(b)\n",
        "        var_img = Image.merge(\"RGB\", (r, g, b))\n",
        "        var_img = var_img.resize((height,width), Image.ANTIALIAS)\n",
        "        var_img = np.array(var_img)\n",
        "        var_img = var_img / 255.0\n",
        "        features.append(var_img)\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), height, width, 3)\n",
        "    features = np.array(features, dtype=np.float64)\n",
        "    return features"
      ],
      "metadata": {
        "id": "d1MGDOVWppjN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import box\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    poly1 = box(box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3])\n",
        "    poly2 = box(box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3])\n",
        "    intersection_area = poly1.intersection(poly2).area\n",
        "    union_area = poly1.union(poly2).area\n",
        "    iou = intersection_area / union_area if union_area > 0 else 0.0\n",
        "    return iou\n",
        "\n",
        "def compute_average_iou(predictions, ground_truths):\n",
        "    total_iou = 0.0\n",
        "    num_predictions = len(predictions)\n",
        "\n",
        "    for i in range(num_predictions):\n",
        "        iou = compute_iou(predictions[i], ground_truths[i])\n",
        "        total_iou += iou\n",
        "\n",
        "    average_iou = total_iou / num_predictions\n",
        "    return average_iou"
      ],
      "metadata": {
        "id": "svrOg6Hw5MhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_acc_graph(history):\n",
        "    acc = history.history[\"accuracy\"]\n",
        "    val_acc = history.history[\"val_accuracy\"]\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc, 'b', label=\"Training Accuracy\")\n",
        "    plt.plot(epochs, val_acc, 'r', label=\"Validation Accuracy\")\n",
        "    plt.title(\"Accuracy Graph\")\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "\n",
        "def display_loss_graph(history):\n",
        "    loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "    epochs = range(len(loss))\n",
        "    plt.plot(epochs, loss, 'b', label=\"Training Loss\")\n",
        "    plt.plot(epochs, val_loss, 'r', label=\"Validation Loss\")\n",
        "    plt.title(\"Loss Graph\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1Qg9vLK1eqF8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "source_path = '/content/kaggle.json'\n",
        "destination_path = '/root/.kaggle/kaggle.json'\n",
        "kaggle_dir = '/root/.kaggle/'\n",
        "if not os.path.exists(kaggle_dir):\n",
        "    os.makedirs(kaggle_dir)\n",
        "os.rename(source_path, destination_path)"
      ],
      "metadata": {
        "id": "fgbKHWLghjgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d andrewmvd/road-sign-detection\n",
        "with zipfile.ZipFile('/content/road-sign-detection.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "_zNRlgwYhaej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_path = '/content/annotations'\n",
        "annotations = os.listdir(annotations_path)"
      ],
      "metadata": {
        "id": "dYHHPo77kMFB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name_list = []\n",
        "width_list = []\n",
        "height_list = []\n",
        "label_list = []\n",
        "xmin_list = []\n",
        "ymin_list = []\n",
        "xmax_list = []\n",
        "ymax_list = []\n",
        "bboxes = []\n",
        "\n",
        "for idx in tqdm(range(len(annotations))):\n",
        "\n",
        "    tree = ET.parse(os.path.join(annotations_path, annotations[idx]))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    img_name = root.find('filename').text\n",
        "\n",
        "    size = root.find('size')\n",
        "    width = size.find('width').text\n",
        "    height = size.find('height').text\n",
        "\n",
        "    for group in root.findall('object'):\n",
        "        label = group.find('name').text\n",
        "        bbox = group.find('bndbox')\n",
        "        xmin = bbox.find('xmin').text\n",
        "        ymin = bbox.find('ymin').text\n",
        "        xmax = bbox.find('xmax').text\n",
        "        ymax = bbox.find('ymax').text\n",
        "\n",
        "        img_name_list.append(\"/content/images/\" + img_name)\n",
        "        width_list.append(width)\n",
        "        height_list.append(height)\n",
        "        xmin_list.append(xmin)\n",
        "        ymin_list.append(ymin)\n",
        "        xmax_list.append(xmax)\n",
        "        ymax_list.append(ymax)\n",
        "        label_list.append(label)\n",
        "        bboxes.append((xmin,ymin,xmax,ymax))"
      ],
      "metadata": {
        "id": "LWl1vRoEi-sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "                        'image_path': img_name_list,\n",
        "                        'width': width_list,\n",
        "                        'height': height_list,\n",
        "                        'xmin': xmin_list,\n",
        "                        'ymin': ymin_list,\n",
        "                        'xmax': xmax_list,\n",
        "                        'ymax': ymax_list,\n",
        "                        'bboxes': bboxes,\n",
        "                        'class_label': label_list})\n",
        "\n",
        "classes = df['class_label'].unique().tolist()\n",
        "classes\n",
        "\n",
        "df['class_int'] = df['class_label'].apply(lambda x: classes.index(x))\n",
        "# print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cDZEJB8njJvw",
        "outputId": "8a63a941-827e-406c-9224-4a1dfa825a86"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    image_path width height xmin ymin xmax ymax  \\\n",
              "0  /content/images/road825.png   300    400    5  335   24  363   \n",
              "1  /content/images/road825.png   300    400  146  169  183  264   \n",
              "2  /content/images/road825.png   300    400  117  209  141  272   \n",
              "3  /content/images/road825.png   300    400  127  104  175  155   \n",
              "4  /content/images/road825.png   300    400  178  178  214  232   \n",
              "\n",
              "                 bboxes   class_label  class_int  \n",
              "0     (5, 335, 24, 363)  trafficlight          0  \n",
              "1  (146, 169, 183, 264)  trafficlight          0  \n",
              "2  (117, 209, 141, 272)  trafficlight          0  \n",
              "3  (127, 104, 175, 155)          stop          1  \n",
              "4  (178, 178, 214, 232)     crosswalk          2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae8566f4-0810-4335-9f18-805ae9bf76c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>bboxes</th>\n",
              "      <th>class_label</th>\n",
              "      <th>class_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/images/road825.png</td>\n",
              "      <td>300</td>\n",
              "      <td>400</td>\n",
              "      <td>5</td>\n",
              "      <td>335</td>\n",
              "      <td>24</td>\n",
              "      <td>363</td>\n",
              "      <td>(5, 335, 24, 363)</td>\n",
              "      <td>trafficlight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/images/road825.png</td>\n",
              "      <td>300</td>\n",
              "      <td>400</td>\n",
              "      <td>146</td>\n",
              "      <td>169</td>\n",
              "      <td>183</td>\n",
              "      <td>264</td>\n",
              "      <td>(146, 169, 183, 264)</td>\n",
              "      <td>trafficlight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/images/road825.png</td>\n",
              "      <td>300</td>\n",
              "      <td>400</td>\n",
              "      <td>117</td>\n",
              "      <td>209</td>\n",
              "      <td>141</td>\n",
              "      <td>272</td>\n",
              "      <td>(117, 209, 141, 272)</td>\n",
              "      <td>trafficlight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/images/road825.png</td>\n",
              "      <td>300</td>\n",
              "      <td>400</td>\n",
              "      <td>127</td>\n",
              "      <td>104</td>\n",
              "      <td>175</td>\n",
              "      <td>155</td>\n",
              "      <td>(127, 104, 175, 155)</td>\n",
              "      <td>stop</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/images/road825.png</td>\n",
              "      <td>300</td>\n",
              "      <td>400</td>\n",
              "      <td>178</td>\n",
              "      <td>178</td>\n",
              "      <td>214</td>\n",
              "      <td>232</td>\n",
              "      <td>(178, 178, 214, 232)</td>\n",
              "      <td>crosswalk</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae8566f4-0810-4335-9f18-805ae9bf76c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae8566f4-0810-4335-9f18-805ae9bf76c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae8566f4-0810-4335-9f18-805ae9bf76c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f00e516b-68e6-41a8-a57f-ff0baa040095\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f00e516b-68e6-41a8-a57f-ff0baa040095')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f00e516b-68e6-41a8-a57f-ff0baa040095 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1244,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 877,\n        \"samples\": [\n          \"/content/images/road692.png\",\n          \"/content/images/road698.png\",\n          \"/content/images/road206.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"268\",\n          \"300\",\n          \"306\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"290\",\n          \"293\",\n          \"302\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"xmin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 246,\n        \"samples\": [\n          \"105\",\n          \"96\",\n          \"231\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ymin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 327,\n        \"samples\": [\n          \"101\",\n          \"302\",\n          \"215\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"xmax\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 280,\n        \"samples\": [\n          \"128\",\n          \"138\",\n          \"71\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ymax\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 287,\n        \"samples\": [\n          \"288\",\n          \"302\",\n          \"219\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bboxes\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1243,\n        \"samples\": [\n          [\n            \"154\",\n            \"63\",\n            \"258\",\n            \"281\"\n          ],\n          [\n            \"107\",\n            \"164\",\n            \"171\",\n            \"230\"\n          ],\n          [\n            \"169\",\n            \"236\",\n            \"178\",\n            \"246\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"stop\",\n          \"speedlimit\",\n          \"trafficlight\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class_int\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "width = 224\n",
        "height = 224\n",
        "og_height = 400\n",
        "og_width = 300\n",
        "\n",
        "width_scale = width / og_width\n",
        "height_scale = height / og_height\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "images_array = []\n",
        "\n",
        "image_paths = np.array(df['image_path'])\n",
        "\n",
        "for image_path in image_paths:\n",
        "    image = load_img(image_path, target_size=(height, width))\n",
        "    images_array.append(np.array(image))\n",
        "\n",
        "class_label_array = np.array(df['class_int'])\n",
        "class_label_array = lb.fit_transform(class_label_array)\n",
        "\n",
        "bbox_array = np.array(df['bboxes'])\n",
        "bbox_array = np.array([list(map(float, box)) for box in bbox_array])\n",
        "for bbox in bbox_array:\n",
        "    bbox[0] *= width_scale\n",
        "    bbox[1] *= height_scale\n",
        "    bbox[2] *= width_scale\n",
        "    bbox[3] *= height_scale\n",
        "\n",
        "images_array, class_label_array, bbox_array, removed_indices = remove_images_with_zero_dimensions(images_array, class_label_array, bbox_array)"
      ],
      "metadata": {
        "id": "2eJ9ikgjne2E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainImages, testImages, trainLabels, testLabels, trainBBoxes, testBBoxes, trainPaths, testPaths = train_test_split(\n",
        "                         images_array,\n",
        "                         class_label_array,\n",
        "                         bbox_array,\n",
        "                         image_paths,\n",
        "                         test_size=0.2,\n",
        "                         random_state=42)"
      ],
      "metadata": {
        "id": "vxXN7vZ2rN5u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_display = [0, 1, 2, 3, 4]\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 4))\n",
        "\n",
        "for i, idx in enumerate(indices_to_display):\n",
        "    image = trainImages[idx]\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(f'Image {idx}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HPOOgETxeHlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception = InceptionV3(weights=\"imagenet\",\n",
        "                        include_top=False,\n",
        "                        input_tensor=Input(shape=(224, 224, 3)))\n",
        "inception.trainable = False\n",
        "flatten = inception.output\n",
        "flatten = Flatten()(flatten)"
      ],
      "metadata": {
        "id": "shez4t_3IsUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = extract_features_contrast(trainImages,height,width)"
      ],
      "metadata": {
        "id": "vxWhWoiC1CGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_display = [0, 1, 2, 3, 4]\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 4))\n",
        "\n",
        "for i, idx in enumerate(indices_to_display):\n",
        "    image = features[idx]\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(f'Image {idx}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dbpwb_7XgfgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bboxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(flatten)\n",
        "bboxHead = Dropout(0.3)(bboxHead)\n",
        "bboxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(bboxHead)\n",
        "bboxHead = Dropout(0.3)(bboxHead)\n",
        "bboxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(bboxHead)\n",
        "bboxHead = Dropout(0.3)(bboxHead)\n",
        "bboxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(bboxHead)\n",
        "bboxHead = Dense(4, activation=\"linear\")(bboxHead)"
      ],
      "metadata": {
        "id": "EPTImRH_oitJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bboxmodel = Model(\n",
        "    inputs=inception.input,\n",
        "    outputs=bboxHead)"
      ],
      "metadata": {
        "id": "v3e2m03lorx_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INIT_LR = 0.0001\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "lr = ReduceLROnPlateau(factor=0.25, patience=3)"
      ],
      "metadata": {
        "id": "HIDS9HPMo3hZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(INIT_LR)\n",
        "\n",
        "bboxmodel.compile(loss=\"mean_absolute_error\",\n",
        "              optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "rkkwoS60o7hj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = bboxmodel.fit(\n",
        "    features, trainBBoxes,\n",
        "    validation_split=0.2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[early_stopping,lr],\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "EEG7E9yKo93n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_acc_graph(history)\n",
        "display_loss_graph(history)"
      ],
      "metadata": {
        "id": "Rb_9G5xHwiM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = extract_features_contrast(testImages,224,224)"
      ],
      "metadata": {
        "id": "UafW_rX26CqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_predictions = bboxmodel.predict(features_test)"
      ],
      "metadata": {
        "id": "0JZnoL-57R3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average IoU:\", compute_average_iou(bbox_predictions, testBBoxes))"
      ],
      "metadata": {
        "id": "1HOHFVmM7_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainImages = crop_images(trainImages, trainBBoxes)"
      ],
      "metadata": {
        "id": "WPg35XorAgrL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = extract_features_contrast_with_padding(trainImages,32,32)"
      ],
      "metadata": {
        "id": "aC3TOrWjAf8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_display = [0, 1]\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "for i, idx in enumerate(indices_to_display):\n",
        "    image = features[idx]\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(f'Image {idx}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dF2Nft3NB-dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNet50(weights=\"imagenet\",\n",
        "                  include_top=False,\n",
        "                  input_tensor=Input(shape=(32, 32, 3)))\n",
        "resnet.trainable = False\n",
        "flatten = resnet.output\n",
        "flatten = Flatten()(flatten)"
      ],
      "metadata": {
        "id": "96c2m0kLAacW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmaxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(flatten)\n",
        "softmaxHead = Dropout(0.3)(softmaxHead)\n",
        "softmaxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(softmaxHead)\n",
        "softmaxHead = Dropout(0.3)(softmaxHead)\n",
        "softmaxHead = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(softmaxHead)\n",
        "softmaxHead = Dropout(0.3)(softmaxHead)\n",
        "softmaxHead = Dense(256, activation=\"relu\")(softmaxHead)\n",
        "softmaxHead = Dense(len(lb.classes_), activation=\"softmax\", name=\"class_label\")(softmaxHead)"
      ],
      "metadata": {
        "id": "PUuP1XaS9WqW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clasificationmodel = Model(\n",
        "    inputs=resnet.input,\n",
        "    outputs=softmaxHead)"
      ],
      "metadata": {
        "id": "cMG8CFmA9e6R"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INIT_LR = 0.0001\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "lr = ReduceLROnPlateau(factor=0.5, patience=5)"
      ],
      "metadata": {
        "id": "9zV1DiTC9kkI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(INIT_LR)\n",
        "\n",
        "clasificationmodel.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "SVjRN1LV9mAs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = clasificationmodel.fit(\n",
        "    features, trainLabels,\n",
        "    validation_split=0.2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[early_stopping,lr],\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "weySY9G39rZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_acc_graph(history)\n",
        "display_loss_graph(history)"
      ],
      "metadata": {
        "id": "RSgsg_JAxr9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testImages = crop_images(testImages, testBBoxes)"
      ],
      "metadata": {
        "id": "bEA8ag_I9mqR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = extract_features_contrast_with_padding(testImages,32,32)"
      ],
      "metadata": {
        "id": "V08WyocJ9iQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clasificationmodel.predict(features_test)\n",
        "\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "testLabels = testLabels.argmax(axis=1)"
      ],
      "metadata": {
        "id": "VSWfjsB1-WdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(testLabels, y_pred)\n",
        "print(classification_report(testLabels, y_pred))"
      ],
      "metadata": {
        "id": "XIozNXqgEO6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix_age = confusion_matrix(testLabels, y_pred)\n",
        "print(conf_matrix_age)\n",
        "\n",
        "skplt.metrics.plot_confusion_matrix(testLabels, y_pred, figsize=(8, 6), cmap='Blues')\n",
        "plt.xlabel('Predicted sign')\n",
        "plt.ylabel('True sign')\n",
        "plt.title('Confusion Matrix for road sign prediction')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vu-ICIQP9U8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df['class_label'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "class_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Counts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2REaMuvCDjeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['width'], bins=30, color='purple', edgecolor='black')\n",
        "plt.title('Distribution of Image Width')\n",
        "plt.xlabel('Width')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['height'], bins=30, color='red', edgecolor='black')\n",
        "plt.title('Distribution of Image Height')\n",
        "plt.xlabel('Height')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdWGLtozp7o5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}